{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5578737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f47bee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Arpit Akar'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60186a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is CEBB-78D3\n",
      "\n",
      " Directory of C:\\Users\\Arpit Akar\n",
      "\n",
      "11-04-2023  23:39    <DIR>          .\n",
      "11-04-2023  23:39    <DIR>          ..\n",
      "10-03-2023  17:30    <DIR>          .astropy\n",
      "10-03-2023  15:01    <DIR>          .conda\n",
      "26-01-2023  09:14    <DIR>          .continuum\n",
      "11-04-2023  22:30    <DIR>          .ipynb_checkpoints\n",
      "26-01-2023  09:15    <DIR>          .ipython\n",
      "27-01-2023  21:45    <DIR>          .jupyter\n",
      "26-01-2023  09:15    <DIR>          .matplotlib\n",
      "15-03-2023  17:57                56 .scala_history\n",
      "29-03-2023  22:58    <DIR>          .spyder-py3\n",
      "21-01-2023  07:39    <DIR>          3D Objects\n",
      "11-04-2023  22:56             7,401 API_programs.ipynb\n",
      "29-03-2023  22:51               453 client.py\n",
      "11-04-2023  18:46            26,511 client_compueter.ipynb\n",
      "21-01-2023  07:39    <DIR>          Contacts\n",
      "12-03-2023  00:07            90,448 COREY_vro.ipynb\n",
      "30-11-2020  00:34             3,243 csv_import_functions.py\n",
      "30-11-2020  00:34               275 Customer Demo.csv\n",
      "30-11-2020  00:34               247 Customer Engagements.csv\n",
      "11-04-2023  18:15               436 customer_contracts.csv\n",
      "11-04-2023  08:58    <DIR>          Desktop\n",
      "27-03-2023  19:53    <DIR>          Documents\n",
      "11-04-2023  19:18    <DIR>          Downloads\n",
      "21-01-2023  07:39    <DIR>          Favorites\n",
      "26-01-2023  14:23            17,821 fisherdata.ipynb\n",
      "23-03-2023  20:19            58,703 Hypothesis testing.ipynb\n",
      "31-01-2023  12:51    <DIR>          Jedi\n",
      "21-01-2023  07:39    <DIR>          Links\n",
      "30-03-2023  12:31             2,204 main.ipynb\n",
      "25-01-2023  03:32    <DIR>          Microsoft\n",
      "21-01-2023  07:39    <DIR>          Music\n",
      "11-04-2023  22:22    <DIR>          OneDrive\n",
      "22-03-2023  19:09    <DIR>          Pictures\n",
      "30-03-2023  12:31             1,792 README.md\n",
      "01-04-2023  23:36           516,616 regression.ipynb\n",
      "21-01-2023  07:39    <DIR>          Saved Games\n",
      "21-01-2023  07:40    <DIR>          Searches\n",
      "29-03-2023  22:51               930 server.py\n",
      "12-03-2023  22:57            31,418 ultratechnse.ipynb\n",
      "25-01-2023  21:53    <DIR>          Videos\n",
      "              16 File(s)        758,554 bytes\n",
      "              25 Dir(s)  166,225,756,160 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b99f318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_files():\n",
    "    csv_files=[]\n",
    "    for file in os.listdir(os.getcwd()):\n",
    "        if file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d86585e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_dataset_directory(csv_files,dataset_dir):\n",
    "    #make directory\n",
    "    try:\n",
    "        mkdir ='mkdir {0}'.format(dataset_dir)\n",
    "        os.system(mkdir)\n",
    "    except:\n",
    "        pass\n",
    "    # move files into directory\n",
    "    for csv in csv_files:\n",
    "        mv_file=\"mv '{0}' {1}\".format(csv,dataset_dir)\n",
    "        os.system(mv_file)\n",
    "        print(mv_file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dba33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir= 'datasets'\n",
    "csvies= csv_files()\n",
    "configure_dataset_directory(csvies,dataset_dir)\n",
    "df=create_df(dataset_dir,csvies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b183f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(dataset_dir,csv_files):\n",
    "    #path to csv files\n",
    "    data_path = os.getcwd()+'/'+dataset_dir+'/'\n",
    "    df={}\n",
    "    # loop through the files and create the dataframe\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df[file]=pd.read_csv(data_path+file)\n",
    "        except UnicodeDecodeError:\n",
    "            df[file]=pd.read_csv(data_path+file,encoding=\"ISO-8859-1\")\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "608b85ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c8e9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tbl_name(filename):\n",
    "    clean_tbl_name=filename.lower().replace(' ','_').replace('$','').replace(r'/','')\\\n",
    "                        .replace('%','')\n",
    "    tbl_name = '{0}'.format(clean_tbl_name.split('.')[0])\n",
    "    # {0} inputing from format split on dot csv and take left part byindexing it to zero\n",
    "    return  tbl_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1da97aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_colname(data_frame):    \n",
    "    data_frame.columns=[x.lower().replace(' ','_').replace('$','').replace('/','')\\\n",
    "                        .replace('%','').replace(r\"(\",\"\") for x in data_frame.columns ]\n",
    "\n",
    "    replacements={\n",
    "        'object':'varchar',\n",
    "        'float64':'float',\n",
    "        'int64':'int',\n",
    "        'datetime64':'timestamp',\n",
    "        'timedelta4[ns]':'varchar'\n",
    "    }\n",
    "        #table schema\n",
    "    col_str=\", \".join(\"{} {}\".format(n,d) for (n,d) in zip(data_frame.columns,data_frame.dtypes.replace(replacements)))\n",
    "    return col_str,data_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b8797a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8184\\2657054476.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m#call dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdata_frame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# clean table name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtbl_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclean_tbl_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "#open a databse conncetion\n",
    "host='http://localhost:8888/notebooks/client_compueter.ipynb'\n",
    "dbname= 'natedb'\n",
    "user= 'nate'\n",
    "password='XXXX'\n",
    "for k in csv_files:\n",
    "    #call dataframe\n",
    "    data_frame=df[k]\n",
    "    # clean table name\n",
    "    tbl_name=clean_tbl_name(k)\n",
    "    # clean column names\n",
    "    col_str,data_frame.columns=clean_columns(data_frame)\n",
    "    upload_to_db(host,dbname,user,password,tbl_name,col_str,file=k,data_frame,data_frame=data_frame.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_db(host,dbname,user,password,tbl_name,col_str,file,data_frame,data_frame=data_frame.columns):\n",
    "    conn_string=\"host=%s \\   #adress to the database\n",
    "                 dbname = %s \\ #db name\n",
    "                 user=%s password=%s \"%(host,dbname,user,password) #db_password\n",
    "    conn= psycopg2.connect(conn_string)\n",
    "    cursor = conn.cursor()\n",
    "    print('opened databse successfully') \n",
    "    #create table\n",
    "     cursor.execute(\"create table %s (%s)\" %(tbl_name,col_str))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    #insert value to table\n",
    "\n",
    "\n",
    "    #save df to csv\n",
    "    df.to_csv('customer_contracts.csv',header=data_frame,index=False,encoding=('utf-8'))\n",
    "    # headers are df.columns and we don't want indexes from pandas df to csv file encoding \n",
    "    #ENCODING during import from exvel there ma be strange symbols that we say that csv file we have have encoding utf-8\n",
    "    #open the csv file\n",
    "    my_file=open[file]\n",
    "    print('file opened in memory')\n",
    "    #upload to db\n",
    "    SQL_STATEMENT= \"\"\"\n",
    "    copy= %s from STDIN WITH \n",
    "            CSV\n",
    "            HEADER\n",
    "            DELIMITER AS ','\n",
    "    \"\"\"\n",
    "    cursor.copy_expert(sql=SQL_STATEMENT,% tbl_name,file=my_file)\n",
    "    print('file copied to db')\n",
    "    cursor.execute('grant select on table %s to public' % tbl_name)\n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "    print('table {0} imported to sb completed'.format(tbl_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
